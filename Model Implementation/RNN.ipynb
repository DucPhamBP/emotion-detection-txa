{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\urbi1\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from utils import *\n",
    "from preprocessing import *\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import  classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocabulary_size, num_classes, embedding_dim, hidden_dim, n_layers, dropout=0.5,weight_decay = 1e-5):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        vocabulary_size (int): The size of the vocabulary (number of unique tokens in the input text).\n",
    "        num_classes (int): The number of output classes (labels).\n",
    "        embedding_dim (int): The dimension of the word embeddings (the vector size representing each word).\n",
    "        hidden_dim (int): The number of units in the hidden state of the RNN.\n",
    "        n_layers (int): The number of RNN layers to stack.\n",
    "        dropout (float, optional): The probability for dropout regularization (default is 0.5).\n",
    "        \"\"\"\n",
    "        super(RNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocabulary_size, embedding_dim)\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim, n_layers, dropout=dropout, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
    "        self.weight_decay = weight_decay \n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        out, hidden = self.rnn(embedded)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc(out[:, -1])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Yangswei_85\n",
    "\n",
    "To use the Yangswei_85 dataset, run the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training set\n",
    "df_train = pd.read_csv('data/train_yangswei_85.csv')  \n",
    "# Load test set\n",
    "test_df = pd.read_csv('data/test_yangswei_85.csv')\n",
    "dataset_name = 'Yangswei_85'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T5 \n",
    "To use the T5 dataset, run the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training set\n",
    "df_train = pd.read_csv('data/train_t5.csv')  \n",
    "# Load test set\n",
    "test_df = pd.read_csv('data/test_t5.csv')\n",
    "dataset_name = 'T5'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocess training data\n",
    "df_train['text'].apply(preprocess).to_frame()\n",
    "\n",
    "# Tokenize and pad training data\n",
    "padded_sequences, train_vocabulary, vocab_size = tokenize_and_pad(df_train[['text']])\n",
    "\n",
    "# Set training data\n",
    "train_data = padded_sequences\n",
    "\n",
    "#Encode labels and save classes in npy file\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(df_train['label'])\n",
    "np.save('data/label_classes.npy', label_encoder.classes_)\n",
    "train_labels = torch.tensor(label_encoder.transform(df_train['label']),dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1/5, Params: {'embedding_dim': 300, 'hidden_dim': 5, 'n_layers': 1, 'dropout': 0.7, 'learning_rate': 0.001, 'weight_decay': 0.001}\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "RNN.__init__() got an unexpected keyword argument 'weight_decay'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 23\u001b[0m\n\u001b[0;32m     13\u001b[0m param_grid \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124membedding_dim\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m300\u001b[39m],  \u001b[38;5;66;03m#200,300\u001b[39;00m\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhidden_dim\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m5\u001b[39m], \u001b[38;5;66;03m#128,256\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweight_decay\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m1e-3\u001b[39m]\n\u001b[0;32m     20\u001b[0m }\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Tune the model hyperparameters\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m best_params, best_accuracy \u001b[38;5;241m=\u001b[39m tune_hyperparams(RNN, model_args, train_data, train_labels, param_grid, epochs, device, patience)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest Parameters: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_params\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_accuracy\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\urbi1\\OneDrive\\Escritorio\\emotion-detection-txa\\Model Implementation\\utils.py:121\u001b[0m, in \u001b[0;36mtune_hyperparams\u001b[1;34m(model_class, model_args, train_data, train_labels, param_grid, epochs, device, patience, k_folds, batch_size)\u001b[0m\n\u001b[0;32m    118\u001b[0m val_loader \u001b[38;5;241m=\u001b[39m DataLoader(TensorDataset(X_val_k, y_val_k), batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    120\u001b[0m \u001b[38;5;66;03m# Initialize model\u001b[39;00m\n\u001b[1;32m--> 121\u001b[0m model \u001b[38;5;241m=\u001b[39m model_class(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_args_with_params)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m    122\u001b[0m initialize_weights(model)\n\u001b[0;32m    123\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mparam_dict\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0.001\u001b[39m), weight_decay\u001b[38;5;241m=\u001b[39mparam_dict\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweight_decay\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;241m1e-5\u001b[39m))\n",
      "\u001b[1;31mTypeError\u001b[0m: RNN.__init__() got an unexpected keyword argument 'weight_decay'"
     ]
    }
   ],
   "source": [
    "#Set fixed model parameters\n",
    "epochs = 500\n",
    "patience = 100\n",
    "num_classes = len(label_encoder.classes_)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model_args = {\n",
    "    'vocabulary_size': vocab_size,\n",
    "    'num_classes': num_classes   \n",
    "}\n",
    "\n",
    "# Set the hyperparameters to tune\n",
    "param_grid = {\n",
    "    'embedding_dim': [300],  #200,300\n",
    "    'hidden_dim': [5], #128,256\n",
    "    'n_layers': [1], #1,2\n",
    "    'dropout': [0.7],  #0.4, 0.5, 0.6\n",
    "    'learning_rate': [0.001],  #0.0001, 0.001, 0.01\n",
    "    'weight_decay': [1e-3]\n",
    "}\n",
    "\n",
    "# Tune the model hyperparameters\n",
    "best_params, best_accuracy = tune_hyperparams(RNN, model_args, train_data, train_labels, param_grid, epochs, device, patience)\n",
    "print(f\"Best Parameters: {best_params}\")\n",
    "print(f\"Best Accuracy: {best_accuracy}\")\n",
    "\n",
    "# Set the best hyperparameters\n",
    "embedding_dim, hidden_dim, n_layers, dropout, learning_rate = best_params.values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set model with best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_model = RNN(vocab_size, num_classes, embedding_dim, hidden_dim, n_layers, dropout).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(rnn_model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Validate best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Start Training and Validation:\\n\")\n",
    "# Split training data into training and validation sets\n",
    "train_loader, val_loader = train_val_split(train_data, train_labels)\n",
    "\n",
    "# Train and validate best model\n",
    "train_losses, val_losses, val_accs, model = train_and_validate(\n",
    "    rnn_model, optimizer, criterion, train_loader, val_loader, epochs, device\n",
    ")\n",
    "\n",
    "# Plot training and validation losses\n",
    "plot_losses(\"RNN\", dataset_name, train_losses, val_losses, epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save best model configuration\n",
    "model_config = {\n",
    "    \"vocab_size\": vocab_size,\n",
    "    \"num_classes\": num_classes,\n",
    "    \"embedding_dim\": embedding_dim,\n",
    "    \"hidden_dim\": hidden_dim,\n",
    "    \"n_layers\": n_layers,\n",
    "    \"dropout\": dropout,\n",
    "    \"state_dict\": model.state_dict()\n",
    "}\n",
    "\n",
    "torch.save(model_config, \"models/RNN.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess test data\n",
    "test_df['text'].apply(preprocess).to_frame()  \n",
    "\n",
    "# Tokenize and pad test data\n",
    "padded_sequences, _, _ = tokenize_and_pad(test_df[['text']], train_vocabulary)\n",
    "\n",
    "# Set test data\n",
    "test_data = padded_sequences\n",
    "\n",
    "#Encode test labels by loading encoder used for training labels\n",
    "label_classes = np.load('data/label_classes.npy', allow_pickle=True)\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.classes_ = label_classes\n",
    "test_labels = torch.tensor(label_encoder.transform(test_df['label']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test best model on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap the test data in a DataLoader\n",
    "test_loader = DataLoader(TensorDataset(test_data, test_labels), batch_size=128, shuffle=False)\n",
    "\n",
    "#Load trained model\n",
    "model_config = torch.load(\"models/RNN.pth\")\n",
    "model = RNN(model_config[\"vocab_size\"], model_config[\"num_classes\"], model_config[\"embedding_dim\"], model_config[\"hidden_dim\"],model_config[\"n_layers\"], model_config[\"dropout\"])  \n",
    "model.load_state_dict(model_config[\"state_dict\"]) \n",
    "model.to(device)\n",
    "\n",
    "#Test model on test set\n",
    "predictions, true_labels = test(model, test_loader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute metrics on model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Compute metrics\n",
    "metrics = compute_metrics(predictions, true_labels)\n",
    "print(f\"Test Accuracy: {metrics['accuracy']:.4f}\")\n",
    "print('\\n')\n",
    "print('Macro Metrics')\n",
    "print(f\"Macro Precision: {metrics['precision']:.4f}\")\n",
    "print(f\"Macro Recall: {metrics['recall']:.4f}\")\n",
    "print(f\"Macro F1 Score: {metrics['f1']:.4f}\")\n",
    "print('\\n')\n",
    "print('Weighted Metrics')\n",
    "print(f\"Weighted Precision: {metrics['precision_weighted']:.4f}\")\n",
    "print(f\"Weighted Recall: {metrics['recall_weighted']:.4f}\")\n",
    "print(f\"Weighted F1 Score: {metrics['f1_weighted']:.4f}\")\n",
    "\n",
    "# Plot confusion matrix\n",
    "plot_confusion_matrix(true_labels, predictions, label_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Print classification report\n",
    "print('Classification Report:\\n')\n",
    "print(classification_report(true_labels, predictions, target_names=label_classes))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "txa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
