{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BiRNN Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\urbi1\\.conda\\envs\\masterDS-exp\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\urbi1\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "C:\\Users\\urbi1\\AppData\\Local\\Temp\\ipykernel_20864\\1163449163.py:12: DeprecationWarning: `import kerastuner` is deprecated, please use `import keras_tuner`.\n",
      "  import kerastuner as kt\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from preprocessing import *\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Dense, Dropout, Bidirectional, LSTM\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "from keras import regularizers\n",
    "import kerastuner as kt\n",
    "from utils import *\n",
    "from metrics_plot_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Yangswei_85\n",
    "\n",
    "To use the Yangswei_85 dataset, run the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training set\n",
    "df_train = pd.read_csv('data/train_yangswei_85.csv')  \n",
    "# Load test set\n",
    "test_df = pd.read_csv('data/test_yangswei_85.csv')\n",
    "dataset_name = 'Yangswei_85'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T5 \n",
    "To use the T5 dataset, run the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training set\n",
    "df_train = pd.read_csv('data/train_t5.csv')  \n",
    "# Load test set\n",
    "test_df = pd.read_csv('data/test_t5.csv')\n",
    "dataset_name = 'T5'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Data Preprocessing\n",
    "\n",
    "Firstly, we apply preprocessing (fixing contractions, setting all text to lowercase, removing non-alphanumeric characters) to the training text.\n",
    "\n",
    "Secondly, we tokenize and pad the training text using Keras' Tokenize, introducing a <UNK> token for unknown entries of the vocabulary.\n",
    "\n",
    "Lastly, we encode the labels of the training dataset and save them to a npy file called 'label_classes.npy'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocess training data\n",
    "df_train['text'].apply(preprocess).to_frame()\n",
    "\n",
    "\n",
    "# Tokenize and pad training data\n",
    "padded_sequences, tokenizer, vocab_size = tokenize_and_pad(df_train[['text']])\n",
    "\n",
    "# Set training data\n",
    "padded_train_data = padded_sequences\n",
    "\n",
    "#Encode labels and save classes in npy file\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(df_train['label'])\n",
    "np.save('data/label_classes.npy', label_encoder.classes_)\n",
    "train_labels = torch.tensor(label_encoder.transform(df_train['label']),dtype=torch.long)\n",
    "num_classes = len(label_encoder.classes_)\n",
    "train_labels_one_hot_encoded = tf.keras.utils.to_categorical(train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading Tuner from .\\untitled_project\\tuner0.json\n",
      "Best hyperparameters: {'output_dim': 128, 'num_layers': 1, 'lstm_units_0': 64, 'dropout_rate_0': 0.4}\n"
     ]
    }
   ],
   "source": [
    "def build_model(hp):\n",
    "    model = Sequential()\n",
    "    # Embedding layer with tunable output dimension\n",
    "    model.add(Embedding(input_dim=vocab_size, output_dim=hp.Choice('output_dim', [128])))#64, 128\n",
    "    # Adjustable number of LSTM layers\n",
    "    num_layers = hp.Int('num_layers', min_value=1, max_value=1, step=1) #1, 2\n",
    "    for i in range(num_layers):\n",
    "        units = hp.Int(f'lstm_units_{i}', min_value=32, max_value=80, step=16) #32, 128, 32\n",
    "        # Add Bidirectional LSTM layer\n",
    "        model.add(\n",
    "            Bidirectional(\n",
    "                LSTM(\n",
    "                    units, \n",
    "                    return_sequences=(i < num_layers - 1),\n",
    "                    kernel_regularizer=regularizers.l2(6e-3) #1e-3\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # Dropout for regularization\n",
    "        model.add(Dropout(hp.Float(f'dropout_rate_{i}', min_value=0.4, max_value=0.6, step=0.1))) # 0.2,  0.5, 0.1\n",
    "    # Output layer for classification\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "tuner = kt.RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=15,  #25\n",
    "    executions_per_trial=1 #2\n",
    ")\n",
    "\n",
    "tuner.search(padded_train_data, train_labels_one_hot_encoded, epochs=15, validation_split=0.2, batch_size=32)\n",
    "\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "print(\"Best hyperparameters:\", best_hps.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Validate best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training and Validation:\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "EarlyStopping.__init__() got an unexpected keyword argument 'restore_best_weights'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 8\u001b[0m\n\u001b[0;32m      4\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m64\u001b[39m\n\u001b[0;32m      7\u001b[0m rnn_model \u001b[38;5;241m=\u001b[39m build_model(best_hps)\n\u001b[1;32m----> 8\u001b[0m early_stopping \u001b[38;5;241m=\u001b[39m EarlyStopping(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, patience\u001b[38;5;241m=\u001b[39mpatience, restore_best_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      9\u001b[0m model_checkpoint \u001b[38;5;241m=\u001b[39m ModelCheckpoint(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodels/best_birnn_model.h5\u001b[39m\u001b[38;5;124m'\u001b[39m, monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, save_best_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin\u001b[39m\u001b[38;5;124m'\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     11\u001b[0m history \u001b[38;5;241m=\u001b[39m rnn_model\u001b[38;5;241m.\u001b[39mfit(\n\u001b[0;32m     12\u001b[0m     padded_train_data, \n\u001b[0;32m     13\u001b[0m     train_labels_one_hot_encoded, \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     18\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     19\u001b[0m )\n",
      "\u001b[1;31mTypeError\u001b[0m: EarlyStopping.__init__() got an unexpected keyword argument 'restore_best_weights'"
     ]
    }
   ],
   "source": [
    "print(\"Start Training and Validation:\\n\")\n",
    "epochs = 50\n",
    "patience = 15\n",
    "batch_size = 64\n",
    "\n",
    "\n",
    "rnn_model = build_model(best_hps)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=patience, restore_best_weights=True)\n",
    "best_model_file_name = f'models/best_birnn_model_{dataset_name}.h5'\n",
    "model_checkpoint = ModelCheckpoint(best_model_file_name, monitor='val_loss', save_best_only=True, mode='min', verbose=1)\n",
    "\n",
    "history = rnn_model.fit(\n",
    "    padded_train_data, \n",
    "    train_labels_one_hot_encoded, \n",
    "    epochs=epochs, \n",
    "    batch_size=batch_size, \n",
    "    validation_split=0.2, \n",
    "    callbacks=[early_stopping, model_checkpoint], \n",
    "    verbose=1\n",
    ")\n",
    "# Extract training and validation losses\n",
    "train_losses = history.history['loss']\n",
    "val_losses = history.history['val_loss']\n",
    "actual_epochs =  len(train_losses)\n",
    "# Plot training and validation losses\n",
    "plot_losses(\"BiRNN\", dataset_name, train_losses, val_losses, actual_epochs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess test data\n",
    "test_df['text'].apply(preprocess).to_frame()  \n",
    "\n",
    "# Tokenize and pad test data using the same tokenizer from training\n",
    "padded_test_sequences = tokenizer.texts_to_sequences(test_df['text'])\n",
    "padded_test_sequences = tf.keras.preprocessing.sequence.pad_sequences(padded_test_sequences, maxlen=padded_train_data.shape[1])\n",
    "\n",
    "# Tokenize and pad test data\n",
    "padded_sequences, _, _ = tokenize_and_pad(test_df[['text']])\n",
    "\n",
    "# Set test data\n",
    "test_data = padded_sequences\n",
    "\n",
    "#Encode test labels by loading encoder used for training labels\n",
    "true_labels = test_df['label']\n",
    "label_classes = np.load('data/label_classes.npy', allow_pickle=True)\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.classes_ = label_classes\n",
    "test_labels = torch.tensor(label_encoder.transform(true_labels))\n",
    "test_labels_one_hot_encoded = tf.keras.utils.to_categorical(test_labels, num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test best model on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the saved model\n",
    "best_model_file_name = f'models/best_birnn_model_{dataset_name}.h5'\n",
    "model = tf.keras.models.load_model(best_model_file_name)\n",
    "\n",
    "\n",
    "test_loss, test_accuracy = model.evaluate(padded_test_sequences, test_labels_one_hot_encoded, verbose=1)\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "\n",
    "predictions_probabilities = model.predict(padded_test_sequences)\n",
    "predicted_classes = label_encoder.inverse_transform(np.argmax(predictions_probabilities, axis=1))\n",
    "true_classes = label_encoder.inverse_transform(np.argmax(test_labels_one_hot_encoded, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute metrics on model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Compute metrics\n",
    "metrics = compute_metrics(predicted_classes, true_classes)\n",
    "print(f\"Test Accuracy: {metrics['accuracy']:.4f}\")\n",
    "print('\\n')\n",
    "print('Macro Metrics')\n",
    "print(f\"Macro Precision: {metrics['precision']:.4f}\")\n",
    "print(f\"Macro Recall: {metrics['recall']:.4f}\")\n",
    "print(f\"Macro F1 Score: {metrics['f1']:.4f}\")\n",
    "print('\\n')\n",
    "print('Weighted Metrics')\n",
    "print(f\"Weighted Precision: {metrics['precision_weighted']:.4f}\")\n",
    "print(f\"Weighted Recall: {metrics['recall_weighted']:.4f}\")\n",
    "print(f\"Weighted F1 Score: {metrics['f1_weighted']:.4f}\")\n",
    "\n",
    "# Plot confusion matrix\n",
    "plot_confusion_matrix(true_classes, predicted_classes, label_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Print classification report\n",
    "print('Classification Report:\\n')\n",
    "print(classification_report(true_classes, predicted_classes, target_names=label_classes))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "txa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
